{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Endpoint Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "- you can check more details on [Artificial Analysis](https://artificialanalysis.ai/methodology) website.\n",
    "- Also check the benchmarking provided by [Databricks](https://docs.databricks.com/en/_extras/notebooks/source/machine-learning/large-language-models/llm-benchmarking.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-9uc9eSHlb9BQo6kDD7qQHJ1BhW4Rq', 'object': 'chat.completion', 'created': 1723280062, 'model': 'gpt-4o-mini-2024-07-18', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'How can I assist you today?', 'refusal': None}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 7, 'total_tokens': 20}, 'system_fingerprint': 'fp_48196bc67a'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"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\"  # Replace with your OpenAI API key\n",
    "endpoint_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "input_tokens = 2048\n",
    "output_tokens = 256\n",
    "num_queries_per_thread = 20\n",
    "\n",
    "# Setup the headers\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Setup the data\n",
    "data = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stop\": [\"\\n\", \"System:\"]\n",
    "}\n",
    "\n",
    "# Make the requests\n",
    "response = requests.post(endpoint_url, headers=headers, json=data)\n",
    "response.raise_for_status()\n",
    "response_json = response.json()\n",
    "print(response_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m out_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m in_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please adjust the input prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mwarm_up_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mwarm_up_and_validate\u001b[0;34m(in_tokens, out_tokens, warm_up_requests)\u001b[0m\n\u001b[1;32m     44\u001b[0m result \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m out_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m in_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please adjust the input prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'usage'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"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\"  # Replace with your OpenAI API key\n",
    "endpoint_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "input_tokens = 2048\n",
    "output_tokens = 256\n",
    "num_queries_per_thread = 20\n",
    "\n",
    "# Setup the headers\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "def get_request(in_tokens, out_tokens):\n",
    "    # Adjust this function to generate a prompt with the specified number of input tokens.\n",
    "    prompt = \" \".join([\"word\"] * in_tokens)\n",
    "    return {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": out_tokens,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stop\": [\"\\n\", \"System:\"]\n",
    "    }\n",
    "\n",
    "# Sends an initial set of warm-up requests and validates the response\n",
    "def warm_up_and_validate(in_tokens=2048, out_tokens=256, warm_up_requests=10):\n",
    "    input_data = get_request(in_tokens, out_tokens)\n",
    "    session = requests.Session()\n",
    "\n",
    "    for _ in range(warm_up_requests):\n",
    "        resp = session.post(endpoint_url, headers=headers, json=input_data)\n",
    "        result = resp.json()\n",
    "        print(result)\n",
    "        assert result['usage']['completion_tokens'] == out_tokens, f\"Model received {result['usage']['completion_tokens']} output tokens, expected {out_tokens}.\"\n",
    "        assert result['usage']['prompt_tokens'] == in_tokens, f\"Model received {result['usage']['prompt_tokens']} input tokens, expected {in_tokens}. Please adjust the input prompt.\"\n",
    "\n",
    "warm_up_and_validate(input_tokens, output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': \"We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please contact us through our help center at help.openai.com.)\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m out_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m in_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please adjust the input prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mwarm_up_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m latencies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# This is a single worker, which processes the given number of requests, one after the other.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mwarm_up_and_validate\u001b[0;34m(in_tokens, out_tokens, warm_up_requests)\u001b[0m\n\u001b[1;32m     44\u001b[0m result \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m out_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m in_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tokens, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please adjust the input prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'usage'"
     ]
    }
   ],
   "source": [
    "\n",
    "latencies = []\n",
    "\n",
    "# This is a single worker, which processes the given number of requests, one after the other.\n",
    "async def worker(index, num_requests, in_tokens=2048, out_tokens=256):\n",
    "    input_data = get_request(in_tokens, out_tokens)\n",
    "    await asyncio.sleep(0.1 * index)\n",
    "\n",
    "    for i in range(num_requests):\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        success = False\n",
    "        while not success:\n",
    "            timeout = aiohttp.ClientTimeout(total=3 * 3600)\n",
    "            async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "                async with session.post(endpoint_url, headers=headers, json=input_data) as response:\n",
    "                    success = response.ok\n",
    "                    chunks = []\n",
    "                    async for chunk, _ in response.content.iter_chunks():\n",
    "                        chunks.append(chunk)\n",
    "        latency = time.time() - request_start_time\n",
    "        result = json.loads(b''.join(chunks))\n",
    "        latencies.append((result['usage']['prompt_tokens'],\n",
    "                          result['usage']['completion_tokens'], latency))\n",
    "\n",
    "# This code runs parallel sets of queries with num_requests_per_worker queries per worker.\n",
    "async def single_benchmark(num_requests_per_worker, num_workers, in_tokens=2048, out_tokens=256):\n",
    "    tasks = []\n",
    "    for i in range(num_workers):\n",
    "        task = asyncio.create_task(worker(i, num_requests_per_worker, in_tokens, out_tokens))\n",
    "        tasks.append(task)\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# This runs the benchmark with 1, n/2 and n output tokens to derive time to first token and time per token.\n",
    "async def benchmark(parallel_queries=1, in_tokens=2048, out_tokens=256, num_tries=5):\n",
    "    avg_num_input_tokens = [0, 0, 0]\n",
    "    avg_num_output_tokens = [0, 0, 0]\n",
    "    median_latency = [0, 0, 0]\n",
    "    print(f\"Parallel queries {parallel_queries}\")\n",
    "\n",
    "    for i, out_tokens in enumerate([1, out_tokens // 2, out_tokens]):\n",
    "        latencies.clear()\n",
    "        await single_benchmark(num_tries, parallel_queries, in_tokens, out_tokens)\n",
    "        avg_num_input_tokens[i] = statistics.mean([inp for inp, _, _ in latencies])\n",
    "        avg_num_output_tokens[i] = statistics.mean([outp for _, outp, _ in latencies])\n",
    "        median_latency[i] = statistics.median([latency for _, _, latency in latencies])\n",
    "        tokens_per_sec = (avg_num_input_tokens[i] + avg_num_output_tokens[i]) * parallel_queries / median_latency[i]\n",
    "        print(f'Output tokens {avg_num_output_tokens[i]}, median latency (s): {round(median_latency[i], 2)}, tokens per second {round(tokens_per_sec, 1)}')\n",
    "\n",
    "    output_token_time = (median_latency[2] - median_latency[1]) * 1000 / (avg_num_output_tokens[2] - avg_num_output_tokens[1])\n",
    "    print(f'Time to first token (s): {round(median_latency[0], 2)}, Time per output token (ms): {round(output_token_time, 2)}')\n",
    "    data.append([median_latency[2],\n",
    "                 (avg_num_input_tokens[2] + avg_num_output_tokens[2]) * parallel_queries / median_latency[2]])\n",
    "\n",
    "# This will run until the throughput of the model is no longer increasing by 10%.\n",
    "data = []\n",
    "for parallel_queries in [1, 2, 4, 8]:\n",
    "    print(f\"Input tokens {input_tokens}\")\n",
    "    await benchmark(parallel_queries, input_tokens, output_tokens, num_queries_per_thread)\n",
    "    if len(data) > 1 and (data[-1][1] - data[-2][1]) / data[-2][1] < 0.1:\n",
    "        break\n",
    "\n",
    "# Plot the latency vs throughput curve\n",
    "plt.xlabel(\"Latency (s)\")\n",
    "plt.ylabel(\"Throughput (tok/s)\")\n",
    "plt.plot([x[0] for x in data], [x[1] for x in data], marker='o')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
